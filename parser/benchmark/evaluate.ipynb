{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.special import comb\n",
    "# import pandas as pd\n",
    "\n",
    "# def evaluate(groundtruth, parsedresult):\n",
    "#     \"\"\"\n",
    "#     Đánh giá độ chính xác Accuracy và F1-score của thuật toán phân tích log bằng cách so sánh dữ liệu ground truth với kết quả đã phân tích.\n",
    "\n",
    "#     Args:\n",
    "#         groundtruth (str): Đường dẫn tới file ground truth (dữ liệu đúng).\n",
    "#         parsedresult (str): Đường dẫn tới file kết quả đã phân tích.\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: (f_measure, accuracy)\n",
    "#             - f_measure (float): Chỉ số F1-score (đánh giá độ chính xác tổng hợp).\n",
    "#             - accuracy (float): Độ chính xác của việc gán nhãn log.\n",
    "#     \"\"\"\n",
    "#     df_groundtruth = pd.read_csv(groundtruth)       # File chính xác\n",
    "#     df_parsedlog = pd.read_csv(parsedresult)        # File kết quả phân tích\n",
    "    \n",
    "#     # Remove invalid groundtruth event Ids\n",
    "#     non_empty_log_ids = df_groundtruth[~df_groundtruth[\"EventId\"].isnull()].index # Lấy index của các dòng không rỗng EventId\n",
    "#     df_groundtruth = df_groundtruth.loc[non_empty_log_ids]  # Lấy các dòng log không rỗng tương ứng trong file chính xác\n",
    "#     df_parsedlog = df_parsedlog.loc[non_empty_log_ids]      # Tương tự\n",
    "    \n",
    "#     (precision, recall, f_measure, accuracy) = get_accuracy(\n",
    "#         df_groundtruth[\"EventId\"], df_parsedlog[\"EventId\"]\n",
    "#     )\n",
    "#     print(\n",
    "#         \"Precision: {:.4f}, Recall: {:.4f}, F1_measure: {:.4f}, Parsing_Accuracy: {:.4f}\".format(\n",
    "#             precision, recall, f_measure, accuracy\n",
    "#         )\n",
    "#     )\n",
    "#     return f_measure, accuracy\n",
    "\n",
    "\n",
    "# def get_accuracy(series_groundtruth: pd.Series, series_parsedlog:pd.Series, debug=True) -> tuple:\n",
    "#     \"\"\"\n",
    "#     Tính toán các chỉ số đánh giá của thuật toán phân tích log.\n",
    "#     `Chú ý`: Giá trị tính toán được xác định là số cặp có trong nhóm log được phân cụm (vì EventID có thể khác nhau giữa file truth và file parse, do đó, cần tính một chỉ số khác).\n",
    "#     Chỉ số được sử dụng không phải tính độ chính xác theo mẫu mà sẽ tính theo số cặp trong nhóm đó.\n",
    "\n",
    "#     Giả sử với n mẫu trong nhóm A, đoán đúng k mẫu, sai (n-k) mẫu.Như vậy, số cặp đoán đúng là:\n",
    "#     TP = C(k, 2) + C(n-k, 2)\n",
    "#     Số cặp đoán sai: k(n-k)\n",
    "#     Số cặp chính xác: C(n, 2)\n",
    "#     Như vậy: TP + Số cặp sai = Số cặp chính xác\n",
    "        \n",
    "#     Args:\n",
    "#         series_groundtruth (pd.Series): Chuỗi `EventId` của dữ liệu ground truth.\n",
    "#         series_parsedlog (pd.Series): Chuỗi `EventId` của kết quả phân tích.\n",
    "#         debug (bool, optional): In thông tin debug khi có lỗi. Mặc định là False.\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: (precision, recall, f_measure, accuracy)\n",
    "#             - precision (float): Độ chính xác (Precision).\n",
    "#             - recall (float): Độ bao phủ (Recall).\n",
    "#             - f_measure (float): Chỉ số F1-score.\n",
    "#             - accuracy (float): Độ chính xác của việc gán nhãn log.\n",
    "#     \"\"\"\n",
    "#     series_groundtruth_valuecounts = series_groundtruth.value_counts()\n",
    "#     real_pairs = 0\n",
    "#     for count in series_groundtruth_valuecounts:\n",
    "#         if count > 1:\n",
    "#             real_pairs += comb(count, 2)\n",
    "\n",
    "#     series_parsedlog_valuecounts = series_parsedlog.value_counts()\n",
    "#     parsed_pairs = 0\n",
    "#     for count in series_parsedlog_valuecounts:\n",
    "#         if count > 1:\n",
    "#             parsed_pairs += comb(count, 2)\n",
    "\n",
    "#     accurate_pairs = 0\n",
    "#     accurate_events = 0  # determine how many lines are correctly parsed\n",
    "#     for parsed_eventId in series_parsedlog_valuecounts.index:\n",
    "#         logIds = series_parsedlog[series_parsedlog == parsed_eventId].index\n",
    "#         series_groundtruth_logId_valuecounts = series_groundtruth[logIds].value_counts()\n",
    "#         error_eventIds = (\n",
    "#             parsed_eventId,\n",
    "#             series_groundtruth_logId_valuecounts.index.tolist(),\n",
    "#         )\n",
    "#         error = True\n",
    "#         if series_groundtruth_logId_valuecounts.size == 1:\n",
    "#             groundtruth_eventId = series_groundtruth_logId_valuecounts.index[0]\n",
    "#             if (\n",
    "#                 logIds.size\n",
    "#                 == series_groundtruth[series_groundtruth == groundtruth_eventId].size\n",
    "#             ):\n",
    "#                 accurate_events += logIds.size\n",
    "#                 error = False\n",
    "#         if error and debug:\n",
    "#             print(\n",
    "#                 \"(parsed_eventId, groundtruth_eventId) =\",\n",
    "#                 error_eventIds,\n",
    "#                 \"failed\",\n",
    "#                 logIds.size,\n",
    "#                 \"messages\",\n",
    "#             )\n",
    "#         for count in series_groundtruth_logId_valuecounts:\n",
    "#             if count > 1:\n",
    "#                 accurate_pairs += comb(count, 2)\n",
    "\n",
    "#     precision = float(accurate_pairs) / parsed_pairs\n",
    "#     recall = float(accurate_pairs) / real_pairs\n",
    "#     f_measure = 2 * precision * recall / (precision + recall)\n",
    "#     accuracy = float(accurate_events) / series_groundtruth.size\n",
    "#     return precision, recall, f_measure, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================== Evaluation on HDFS ========================\n",
      "Parsing file: ../../data_benchmark/loghub_2k/HDFS\\HDFS_2k.log\n",
      "Total lines:  2000\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.186829]\n",
      "(parsed_eventId, groundtruth_eventId) = ('04137b95', ['E4']) failed 2 messages\n",
      "(parsed_eventId, groundtruth_eventId) = ('98012f03', ['E4']) failed 2 messages\n",
      "(parsed_eventId, groundtruth_eventId) = ('415a1760', ['E4']) failed 1 messages\n",
      "Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 0.9975\n",
      "\n",
      "======================== Evaluation on BGL ========================\n",
      "Parsing file: ../../data_benchmark/loghub_2k/BGL\\BGL_2k.log\n",
      "Total lines:  2000\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.170242]\n",
      "(parsed_eventId, groundtruth_eventId) = ('cc8ff6de', ['E101', 'E103', 'E102', 'E105', 'E104']) failed 21 messages\n",
      "(parsed_eventId, groundtruth_eventId) = ('ad69fd32', ['E32', 'E30']) failed 12 messages\n",
      "(parsed_eventId, groundtruth_eventId) = ('9851467f', ['E53', 'E54']) failed 10 messages\n",
      "(parsed_eventId, groundtruth_eventId) = ('9d143d9b', ['E78']) failed 8 messages\n",
      "(parsed_eventId, groundtruth_eventId) = ('d6e5f01c', ['E10', 'E8', 'E9']) failed 4 messages\n",
      "(parsed_eventId, groundtruth_eventId) = ('55addd29', ['E69', 'E68']) failed 3 messages\n",
      "(parsed_eventId, groundtruth_eventId) = ('4496b375', ['E23', 'E21', 'E22']) failed 3 messages\n",
      "(parsed_eventId, groundtruth_eventId) = ('d9ec2e41', ['E73']) failed 3 messages\n",
      "(parsed_eventId, groundtruth_eventId) = ('825e06cc', ['E46', 'E47']) failed 3 messages\n",
      "(parsed_eventId, groundtruth_eventId) = ('3777cdd1', ['E72', 'E71']) failed 3 messages\n",
      "(parsed_eventId, groundtruth_eventId) = ('2ba1baf7', ['E44', 'E45']) failed 2 messages\n",
      "(parsed_eventId, groundtruth_eventId) = ('34832905', ['E73']) failed 2 messages\n",
      "(parsed_eventId, groundtruth_eventId) = ('d9735fda', ['E78']) failed 1 messages\n",
      "Precision: 0.9992, Recall: 1.0000, F1_measure: 0.9996, Parsing_Accuracy: 0.9625\n",
      "\n",
      "======================== Overall evaluation results ========================\n",
      "         F1_measure  Accuracy\n",
      "Dataset                      \n",
      "HDFS       0.999984    0.9975\n",
      "BGL        0.999599    0.9625\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import Drain_setting\n",
    "sys.path.append(\"../../parser\")\n",
    "\n",
    "from utils import evaluator\n",
    "\n",
    "import root.Drain.Drain\n",
    "import importlib\n",
    "\n",
    "importlib.reload(root.Drain.Drain)  # Reload module\n",
    "LogParser = root.Drain.Drain.LogParser  # Truy cập class LogParser\n",
    "\n",
    "\n",
    "assign_dataset = [\"HDFS\", \"BGL\"]\n",
    "benchmark_settings = {}\n",
    "for temp in assign_dataset:\n",
    "    benchmark_settings[temp] = Drain_setting.BENCHMARK_DRAIN_SETTING[temp]\n",
    "input_dir = '../../data_benchmark/loghub_2k/'\n",
    "output_dir = '../result_data/Drain/'\n",
    "\n",
    "\n",
    "bechmark_result = []\n",
    "for dataset, setting in benchmark_settings.items():\n",
    "    print(\"\\n======================== Evaluation on %s ========================\" % dataset)\n",
    "    indir = os.path.join(input_dir, os.path.dirname(setting[\"log_file\"]))\n",
    "    log_file = os.path.basename(setting[\"log_file\"])\n",
    "\n",
    "    parser = LogParser(\n",
    "        log_format=setting[\"log_format\"],\n",
    "        indir=indir,\n",
    "        outdir=output_dir,\n",
    "        rex=setting[\"regex\"],\n",
    "        depth=setting[\"depth\"],\n",
    "        st=setting[\"st\"],\n",
    "    )\n",
    "    parser.parse(log_file)\n",
    "\n",
    "    F1_measure, accuracy = evaluator.evaluate(\n",
    "        groundtruth=os.path.join(indir, log_file + \"_structured.csv\"),\n",
    "        parsedresult=os.path.join(output_dir, log_file + \"_structured.csv\"),\n",
    "    )\n",
    "    bechmark_result.append([dataset, F1_measure, accuracy])\n",
    "\n",
    "\n",
    "print(\"\\n======================== Overall evaluation results ========================\")\n",
    "df_result = pd.DataFrame(bechmark_result, columns=[\"Dataset\", \"F1_measure\", \"Accuracy\"])\n",
    "df_result.set_index(\"Dataset\", inplace=True)\n",
    "print(df_result)\n",
    "df_result.to_csv(\"Drain_bechmark_result.csv\", float_format=\"%.6f\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LOGKL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
