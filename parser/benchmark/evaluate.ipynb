{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.special import comb\n",
    "# import pandas as pd\n",
    "\n",
    "# def evaluate(groundtruth, parsedresult):\n",
    "#     \"\"\"\n",
    "#     Đánh giá độ chính xác Accuracy và F1-score của thuật toán phân tích log bằng cách so sánh dữ liệu ground truth với kết quả đã phân tích.\n",
    "\n",
    "#     Args:\n",
    "#         groundtruth (str): Đường dẫn tới file ground truth (dữ liệu đúng).\n",
    "#         parsedresult (str): Đường dẫn tới file kết quả đã phân tích.\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: (f_measure, accuracy)\n",
    "#             - f_measure (float): Chỉ số F1-score (đánh giá độ chính xác tổng hợp).\n",
    "#             - accuracy (float): Độ chính xác của việc gán nhãn log.\n",
    "#     \"\"\"\n",
    "#     df_groundtruth = pd.read_csv(groundtruth)       # File chính xác\n",
    "#     df_parsedlog = pd.read_csv(parsedresult)        # File kết quả phân tích\n",
    "    \n",
    "#     # Remove invalid groundtruth event Ids\n",
    "#     non_empty_log_ids = df_groundtruth[~df_groundtruth[\"EventId\"].isnull()].index # Lấy index của các dòng không rỗng EventId\n",
    "#     df_groundtruth = df_groundtruth.loc[non_empty_log_ids]  # Lấy các dòng log không rỗng tương ứng trong file chính xác\n",
    "#     df_parsedlog = df_parsedlog.loc[non_empty_log_ids]      # Tương tự\n",
    "    \n",
    "#     (precision, recall, f_measure, accuracy) = get_accuracy(\n",
    "#         df_groundtruth[\"EventId\"], df_parsedlog[\"EventId\"]\n",
    "#     )\n",
    "#     print(\n",
    "#         \"Precision: {:.4f}, Recall: {:.4f}, F1_measure: {:.4f}, Parsing_Accuracy: {:.4f}\".format(\n",
    "#             precision, recall, f_measure, accuracy\n",
    "#         )\n",
    "#     )\n",
    "#     return f_measure, accuracy\n",
    "\n",
    "\n",
    "# def get_accuracy(series_groundtruth: pd.Series, series_parsedlog:pd.Series, debug=True) -> tuple:\n",
    "#     \"\"\"\n",
    "#     Tính toán các chỉ số đánh giá của thuật toán phân tích log.\n",
    "#     `Chú ý`: Giá trị tính toán được xác định là số cặp có trong nhóm log được phân cụm (vì EventID có thể khác nhau giữa file truth và file parse, do đó, cần tính một chỉ số khác).\n",
    "#     Chỉ số được sử dụng không phải tính độ chính xác theo mẫu mà sẽ tính theo số cặp trong nhóm đó.\n",
    "\n",
    "#     Giả sử với n mẫu trong nhóm A, đoán đúng k mẫu, sai (n-k) mẫu.Như vậy, số cặp đoán đúng là:\n",
    "#     TP = C(k, 2) + C(n-k, 2)\n",
    "#     Số cặp đoán sai: k(n-k)\n",
    "#     Số cặp chính xác: C(n, 2)\n",
    "#     Như vậy: TP + Số cặp sai = Số cặp chính xác\n",
    "        \n",
    "#     Args:\n",
    "#         series_groundtruth (pd.Series): Chuỗi `EventId` của dữ liệu ground truth.\n",
    "#         series_parsedlog (pd.Series): Chuỗi `EventId` của kết quả phân tích.\n",
    "#         debug (bool, optional): In thông tin debug khi có lỗi. Mặc định là False.\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: (precision, recall, f_measure, accuracy)\n",
    "#             - precision (float): Độ chính xác (Precision).\n",
    "#             - recall (float): Độ bao phủ (Recall).\n",
    "#             - f_measure (float): Chỉ số F1-score.\n",
    "#             - accuracy (float): Độ chính xác của việc gán nhãn log.\n",
    "#     \"\"\"\n",
    "#     series_groundtruth_valuecounts = series_groundtruth.value_counts()\n",
    "#     real_pairs = 0\n",
    "#     for count in series_groundtruth_valuecounts:\n",
    "#         if count > 1:\n",
    "#             real_pairs += comb(count, 2)\n",
    "\n",
    "#     series_parsedlog_valuecounts = series_parsedlog.value_counts()\n",
    "#     parsed_pairs = 0\n",
    "#     for count in series_parsedlog_valuecounts:\n",
    "#         if count > 1:\n",
    "#             parsed_pairs += comb(count, 2)\n",
    "\n",
    "#     accurate_pairs = 0\n",
    "#     accurate_events = 0  # determine how many lines are correctly parsed\n",
    "#     for parsed_eventId in series_parsedlog_valuecounts.index:\n",
    "#         logIds = series_parsedlog[series_parsedlog == parsed_eventId].index\n",
    "#         series_groundtruth_logId_valuecounts = series_groundtruth[logIds].value_counts()\n",
    "#         error_eventIds = (\n",
    "#             parsed_eventId,\n",
    "#             series_groundtruth_logId_valuecounts.index.tolist(),\n",
    "#         )\n",
    "#         error = True\n",
    "#         if series_groundtruth_logId_valuecounts.size == 1:\n",
    "#             groundtruth_eventId = series_groundtruth_logId_valuecounts.index[0]\n",
    "#             if (\n",
    "#                 logIds.size\n",
    "#                 == series_groundtruth[series_groundtruth == groundtruth_eventId].size\n",
    "#             ):\n",
    "#                 accurate_events += logIds.size\n",
    "#                 error = False\n",
    "#         if error and debug:\n",
    "#             print(\n",
    "#                 \"(parsed_eventId, groundtruth_eventId) =\",\n",
    "#                 error_eventIds,\n",
    "#                 \"failed\",\n",
    "#                 logIds.size,\n",
    "#                 \"messages\",\n",
    "#             )\n",
    "#         for count in series_groundtruth_logId_valuecounts:\n",
    "#             if count > 1:\n",
    "#                 accurate_pairs += comb(count, 2)\n",
    "\n",
    "#     precision = float(accurate_pairs) / parsed_pairs\n",
    "#     recall = float(accurate_pairs) / real_pairs\n",
    "#     f_measure = 2 * precision * recall / (precision + recall)\n",
    "#     accuracy = float(accurate_events) / series_groundtruth.size\n",
    "#     return precision, recall, f_measure, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================== Model: Drain ===============================\n",
      "\n",
      "------------------------- Evaluation on HDFS -------------------------\n",
      "Parsing file: ../../data_bechmark/loghub_2k_corrected/HDFS\\HDFS_2k.log\n",
      "Total lines:  2000\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.218844]\n",
      "Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 0.9975\n",
      "\n",
      "------------------------- Evaluation on BGL -------------------------\n",
      "Parsing file: ../../data_bechmark/loghub_2k_corrected/BGL\\BGL_2k.log\n",
      "Total lines:  2000\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.183397]\n",
      "Precision: 0.9992, Recall: 1.0000, F1_measure: 0.9996, Parsing_Accuracy: 0.9625\n",
      "\n",
      "------------------------- Evaluation on Hadoop -------------------------\n",
      "Parsing file: ../../data_bechmark/loghub_2k_corrected/Hadoop\\Hadoop_2k.log\n",
      "Total lines:  2000\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.209564]\n",
      "Precision: 0.9982, Recall: 0.9998, F1_measure: 0.9990, Parsing_Accuracy: 0.9475\n",
      "\n",
      "------------------------- Evaluation on OpenStack -------------------------\n",
      "Parsing file: ../../data_bechmark/loghub_2k_corrected/OpenStack\\OpenStack_2k.log\n",
      "Total lines:  2000\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.387839]\n",
      "Precision: 0.9992, Recall: 0.9860, F1_measure: 0.9925, Parsing_Accuracy: 0.7325\n",
      "\n",
      "------------------------- Evaluation on Thunderbird -------------------------\n",
      "Parsing file: ../../data_bechmark/loghub_2k_corrected/Thunderbird\\Thunderbird_2k.log\n",
      "Total lines:  2000\n",
      "Processed 100.0% of log lines.\n",
      "Parsing done. [Time taken: 0:00:00.219463]\n",
      "Precision: 0.9998, Recall: 0.9988, F1_measure: 0.9993, Parsing_Accuracy: 0.9550\n",
      "\n",
      "----------------------- Overall evaluation results ------------------------\n",
      "             F1_measure  Accuracy      Time\n",
      "Dataset                                    \n",
      "HDFS           0.999984    0.9975  0.218844\n",
      "BGL            0.999599    0.9625  0.183977\n",
      "Hadoop         0.998989    0.9475  0.209564\n",
      "OpenStack      0.992536    0.7325  0.388838\n",
      "Thunderbird    0.999329    0.9550  0.219463\n",
      "\n",
      "==================================== END ====================================\n",
      "\n",
      "=============================== Model: Brain ===============================\n",
      "\n",
      "------------------------- Evaluation on HDFS -------------------------\n",
      "Parsing file: ../../data_bechmark/loghub_2k_corrected/HDFS\\HDFS_2k.log\n",
      "Parsing done...\n",
      "Time taken   =   \u001b[38;2;255;192;203m0:00:00.164705\u001b[0m\n",
      "Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 0.9975\n",
      "\n",
      "------------------------- Evaluation on BGL -------------------------\n",
      "Parsing file: ../../data_bechmark/loghub_2k_corrected/BGL\\BGL_2k.log\n",
      "Parsing done...\n",
      "Time taken   =   \u001b[38;2;255;192;203m0:00:00.166853\u001b[0m\n",
      "Precision: 1.0000, Recall: 0.9999, F1_measure: 0.9999, Parsing_Accuracy: 0.9860\n",
      "\n",
      "------------------------- Evaluation on Hadoop -------------------------\n",
      "Parsing file: ../../data_bechmark/loghub_2k_corrected/Hadoop\\Hadoop_2k.log\n",
      "Parsing done...\n",
      "Time taken   =   \u001b[38;2;255;192;203m0:00:00.207714\u001b[0m\n",
      "Precision: 1.0000, Recall: 0.9975, F1_measure: 0.9987, Parsing_Accuracy: 0.9490\n",
      "\n",
      "------------------------- Evaluation on OpenStack -------------------------\n",
      "Parsing file: ../../data_bechmark/loghub_2k_corrected/OpenStack\\OpenStack_2k.log\n",
      "Parsing done...\n",
      "Time taken   =   \u001b[38;2;255;192;203m0:00:00.232120\u001b[0m\n",
      "Precision: 1.0000, Recall: 1.0000, F1_measure: 1.0000, Parsing_Accuracy: 1.0000\n",
      "\n",
      "------------------------- Evaluation on Thunderbird -------------------------\n",
      "Parsing file: ../../data_bechmark/loghub_2k_corrected/Thunderbird\\Thunderbird_2k.log\n",
      "Parsing done...\n",
      "Time taken   =   \u001b[38;2;255;192;203m0:00:00.162039\u001b[0m\n",
      "Precision: 1.0000, Recall: 0.9999, F1_measure: 0.9999, Parsing_Accuracy: 0.9710\n",
      "\n",
      "----------------------- Overall evaluation results ------------------------\n",
      "             F1_measure  Accuracy      Time\n",
      "Dataset                                    \n",
      "HDFS           0.999984    0.9975   0.18192\n",
      "BGL            0.999932    0.9860  0.184051\n",
      "Hadoop         0.998749    0.9490   0.22872\n",
      "OpenStack      1.000000    1.0000  0.252462\n",
      "Thunderbird    0.999933    0.9710  0.181008\n",
      "\n",
      "==================================== END ====================================\n",
      "\n",
      "=============================== Model: Logram ===============================\n",
      "\n",
      "------------------------- Evaluation on HDFS -------------------------\n",
      "Parsing file: ../../data_bechmark/loghub_2k_corrected/HDFS\\HDFS_2k.log\n",
      "Parsing done.\n",
      "Precision: 1.0000, Recall: 0.9812, F1_measure: 0.9905, Parsing_Accuracy: 0.9300\n",
      "\n",
      "------------------------- Evaluation on BGL -------------------------\n",
      "Parsing file: ../../data_bechmark/loghub_2k_corrected/BGL\\BGL_2k.log\n",
      "Parsing done.\n",
      "Precision: 0.9969, Recall: 0.9183, F1_measure: 0.9560, Parsing_Accuracy: 0.5870\n",
      "\n",
      "------------------------- Evaluation on Hadoop -------------------------\n",
      "Parsing file: ../../data_bechmark/loghub_2k_corrected/Hadoop\\Hadoop_2k.log\n",
      "Parsing done.\n",
      "Precision: 0.9977, Recall: 0.6437, F1_measure: 0.7825, Parsing_Accuracy: 0.4510\n",
      "\n",
      "------------------------- Evaluation on OpenStack -------------------------\n",
      "Parsing file: ../../data_bechmark/loghub_2k_corrected/OpenStack\\OpenStack_2k.log\n",
      "Parsing done.\n",
      "Precision: 0.9788, Recall: 0.5986, F1_measure: 0.7429, Parsing_Accuracy: 0.3255\n",
      "\n",
      "------------------------- Evaluation on Thunderbird -------------------------\n",
      "Parsing file: ../../data_bechmark/loghub_2k_corrected/Thunderbird\\Thunderbird_2k.log\n",
      "Parsing done.\n",
      "Precision: 0.9903, Recall: 0.9974, F1_measure: 0.9939, Parsing_Accuracy: 0.5540\n",
      "\n",
      "----------------------- Overall evaluation results ------------------------\n",
      "             F1_measure  Accuracy      Time\n",
      "Dataset                                    \n",
      "HDFS           0.990518    0.9300   0.10908\n",
      "BGL            0.956032    0.5870  0.068471\n",
      "Hadoop         0.782490    0.4510  0.098741\n",
      "OpenStack      0.742866    0.3255  0.112604\n",
      "Thunderbird    0.993876    0.5540  0.107649\n",
      "\n",
      "==================================== END ====================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import importlib\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../parser\")\n",
    "\n",
    "from utils import evaluator\n",
    "\n",
    "import Drain_setting\n",
    "import Brain_setting\n",
    "import Logram_setting\n",
    "\n",
    "import root.Drain.Drain as drain\n",
    "import root.Brain.Brain as brain\n",
    "import root.Logram.Logram as logram\n",
    "\n",
    "importlib.reload(Drain_setting)  # Reload module\n",
    "importlib.reload(Brain_setting)  # Reload module        \n",
    "importlib.reload(Logram_setting)  # Reload module\n",
    "\n",
    "importlib.reload(drain)  # Reload module\n",
    "importlib.reload(brain)  # Reload module\n",
    "importlib.reload(logram)  # Reload module\n",
    "\n",
    "def get_bechmark_data(assign_dataset, root_source):\n",
    "    bechmark_settings = {}\n",
    "    for temp in assign_dataset:\n",
    "        bechmark_settings[temp] = root_source[temp]\n",
    "    return copy.deepcopy(bechmark_settings)\n",
    "\n",
    "def get_parser(name_model, setting, indir, output_dir, dataset):\n",
    "    match name_model.upper():\n",
    "        case \"DRAIN\":\n",
    "            return drain.LogParser(\n",
    "                log_format=setting[\"log_format\"],\n",
    "                indir=indir,\n",
    "                outdir=output_dir,\n",
    "                rex=setting[\"regex\"],\n",
    "                depth=setting[\"depth\"],\n",
    "                st=setting[\"st\"],\n",
    "            )\n",
    "        case \"BRAIN\":\n",
    "            return brain.LogParser(\n",
    "                log_format=setting[\"log_format\"],\n",
    "                indir=indir,\n",
    "                outdir=output_dir,\n",
    "                rex=setting[\"regex\"],\n",
    "                delimeter=setting[\"delimiter\"],\n",
    "                threshold=setting[\"theshold\"],\n",
    "                logname=dataset,\n",
    "            )\n",
    "        case \"LOGRAM\":\n",
    "            return logram.LogParser(\n",
    "                log_format=setting[\"log_format\"],\n",
    "                indir=indir,\n",
    "                outdir=output_dir,\n",
    "                rex=setting[\"regex\"],\n",
    "                doubleThreshold=setting[\"doubleThreshold\"],\n",
    "                triThreshold=setting[\"triThreshold\"],\n",
    "            )\n",
    "        case _:\n",
    "            return None\n",
    "\n",
    "def get_rating(name_model, config_data):\n",
    "    bechmark_result = []\n",
    "    bechmark_set = config_data[\"bechmark\"]\n",
    "    \n",
    "    # Chạy trên từng loại dataset:\n",
    "    for dataset, setting in bechmark_set.items():\n",
    "        print(\"\\n------------------------- Evaluation on %s -------------------------\" % dataset)\n",
    "        indir = os.path.join(config_data[\"input_dir\"], os.path.dirname(setting[\"log_file\"]))\n",
    "        log_file = os.path.basename(setting[\"log_file\"])\n",
    "\n",
    "        parser = get_parser(name_model, setting, indir, config_data[\"output_dir\"], dataset)\n",
    "        assert(parser is not None)    # \"Model không tồn tại\"\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        parser.parse(log_file)\n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        time_elapsed = end_time - start_time\n",
    "        time_elapsed = str(time_elapsed.total_seconds())\n",
    "        \n",
    "        F1_measure, accuracy = evaluator.evaluate(\n",
    "            groundtruth=os.path.join(config_data[\"input_dir\"] + dataset + \"/\", log_file + \"_structured.csv\"),\n",
    "            parsedresult=os.path.join(config_data[\"output_dir\"], log_file + \"_structured.csv\"),\n",
    "        )\n",
    "        bechmark_result.append([dataset, F1_measure, accuracy, time_elapsed])\n",
    "\n",
    "    print(\"\\n----------------------- Overall evaluation results ------------------------\")\n",
    "    df_result = pd.DataFrame(bechmark_result, columns=[\"Dataset\", \"F1_measure\", \"Accuracy\", \"Time\"])\n",
    "    df_result.set_index(\"Dataset\", inplace=True)\n",
    "    print(df_result)\n",
    "    df_result.to_csv(\"Drain_bechmark_result.csv\", float_format=\"%.6f\")\n",
    "    return bechmark_result\n",
    "\n",
    "# Lựa chọn dataset nào để thực hiện đánh giá bechmark:\n",
    "# [\"HDFS\", \"BGL\", \"Hadoop\", \"Zookeeper\", \"Proxifier\", \"Windows\", \"Andriod\", \"HealthApp\", \"Spark\", \"Thunderbird\", \"HPC\", \"Linux\",\"OpenSSH\", \"OpenStack\", \"Mac\", \"Apache\"]\n",
    "assign_dataset = [\"HDFS\", \"BGL\", \"Hadoop\", \"OpenStack\", \"Thunderbird\"]\n",
    "\n",
    "MODEL_BECHMARK_SETTINGS = {\n",
    "   \"Drain\": {\n",
    "        \"bechmark\" : get_bechmark_data(assign_dataset, Drain_setting.BECHMARK_DRAIN_SETTINGS),\n",
    "        \"input_dir\" : '../../data_bechmark/loghub_2k_corrected/',\n",
    "        \"output_dir\": '../result_data/Drain/',\n",
    "   },\n",
    "   \"Brain\": {\n",
    "        \"bechmark\" : get_bechmark_data(assign_dataset, Brain_setting.BECHMARK_BRAIN_SETTINGS),\n",
    "        \"input_dir\" : '../../data_bechmark/loghub_2k_corrected/',\n",
    "        \"output_dir\": '../result_data/Brain/',\n",
    "   },\n",
    "    \"Logram\": {\n",
    "          \"bechmark\" : get_bechmark_data(assign_dataset, Logram_setting.BECHMARK_LOGRAM_SETTINGS),\n",
    "          \"input_dir\" : '../../data_bechmark/loghub_2k_corrected/',\n",
    "          \"output_dir\": '../result_data/Logram/',\n",
    "    } \n",
    "}\n",
    "\n",
    "result_rating = {}\n",
    "for name_model, config_data in MODEL_BECHMARK_SETTINGS.items():\n",
    "    print(f\"\\n=============================== Model: {name_model} ===============================\")\n",
    "    \n",
    "    result_rating[\"name_model\"] = get_rating(name_model, config_data)\n",
    "    print(\"\\n==================================== END ====================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LOGKL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
